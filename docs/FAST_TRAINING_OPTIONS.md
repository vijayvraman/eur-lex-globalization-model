# Fast CPT Training Options: 4-6 Hours

Strategies to reduce CPT training time from 20-24 hours to 4-6 hours (75-80% reduction).

---

## üöÄ Quick Start (Recommended)

**Want to get started immediately?** Use the automated workflow script:

```bash
# Run the complete fast CPT workflow (filtering + corpus building + training)
./scripts/run_fast_cpt_training.sh
```

This script implements the **Hybrid Approach** (Option 6, most recommended) and handles:
- ‚úÖ Document filtering (top 40% quality, ~2B tokens)
- ‚úÖ CPT corpus building (3072-token sequences, 16 shards)
- ‚úÖ Fast CPT training (12,000 steps, 4-6 hours)
- ‚úÖ Automatic GPU detection and environment setup

**Configuration file**: `configs/cpt_config_fast.yaml`

**Skip to training only** (if you already filtered and built corpus):
```bash
./scripts/run_fast_cpt_training.sh skip skip
```

For detailed explanation of all options and trade-offs, continue reading below.

---

## Option 1: Reduced Training Steps (Recommended) ‚≠ê

### Configuration
- **Current**: 40,000 steps (~5B tokens)
- **Reduced**: 10,000 steps (~1.25B tokens)
- **Time**: 5-6 hours
- **Trade-off**: Less domain adaptation

### Implementation
```yaml
# configs/cpt_config_fast.yaml
training:
  max_steps: 10000  # Instead of 40000
  warmup_steps: 500  # Reduced proportionally
```

### Pros
- ‚úÖ Simplest to implement
- ‚úÖ Still gets significant domain adaptation
- ‚úÖ No additional infrastructure needed
- ‚úÖ 75% time reduction

### Cons
- ‚ö†Ô∏è Less comprehensive legal knowledge
- ‚ö†Ô∏è May need longer SFT phase to compensate
- ‚ö†Ô∏è ~60-70% of full CPT benefit (still valuable)

### Best For
- Rapid prototyping
- Budget-conscious projects
- Time-sensitive deployments
- Testing before full training

---

## Option 2: Reduce Corpus Size

### Configuration
- **Current**: ~5B tokens (20GB corpus)
- **Reduced**: ~1.5B tokens (6GB corpus, top 30% quality)
- **Time**: 6 hours
- **Trade-off**: Less data coverage

### Implementation
```python
# In cpt_corpus_builder.py
def filter_high_quality_documents(documents):
    """Keep only highest quality documents"""
    # Criteria:
    # - Recent documents (2020+)
    # - Regulations and directives (not opinions)
    # - Longer documents (>500 tokens)
    # - Key legal areas (GDPR, data protection, consumer rights)

    filtered = []
    for doc in documents:
        metadata = doc.get('metadata', {})

        # Filter by date
        date = metadata.get('date', '')
        if date and int(date[:4]) < 2020:
            continue

        # Filter by document type
        doc_type = metadata.get('doc_type', '')
        if doc_type not in ['regulation', 'directive']:
            continue

        # Filter by length
        if len(doc.get('full_text', '')) < 500:
            continue

        filtered.append(doc)

    return filtered
```

### Pros
- ‚úÖ Focuses on most relevant content
- ‚úÖ Better quality-to-time ratio
- ‚úÖ Reduces storage requirements

### Cons
- ‚ö†Ô∏è Manual curation effort
- ‚ö†Ô∏è May miss niche legal areas
- ‚ö†Ô∏è Less multilingual coverage

---

## Option 3: Increase GPU Count (4x ‚Üí 16x)

### Configuration
- **Current**: 4x B200 GPUs
- **Increased**: 16x B200 GPUs
- **Time**: 5-6 hours (4x speedup)
- **Trade-off**: 4x higher cost

### Implementation
```bash
# Update DeepSpeed config
{
  "train_batch_size": 512,  # 4x larger
  "train_micro_batch_size_per_gpu": 2,
  "gradient_accumulation_steps": 16
}

# Launch with 16 GPUs
deepspeed --num_gpus=16 scripts/train_cpt.py
```

### Pros
- ‚úÖ Full training quality maintained
- ‚úÖ No compromise on domain adaptation
- ‚úÖ Linear scaling (4x GPUs = ~4x faster)

### Cons
- ‚ö†Ô∏è 4x higher GPU cost (~$2000-2400 vs $500-600)
- ‚ö†Ô∏è Requires 16-GPU cluster access
- ‚ö†Ô∏è More complex setup and debugging

### Cost Comparison
- 4 GPUs @ 20h = $500 (current)
- 16 GPUs @ 5h = $2000 (this option)
- **Extra cost**: $1500

---

## Option 4: LoRA Instead of Full CPT ‚≠ê‚≠ê

### Configuration
- **Method**: Low-Rank Adaptation (LoRA)
- **Parameters trained**: ~0.5% of model (350M params)
- **Time**: 4-6 hours
- **Trade-off**: Adapter-based, not full model update

### Implementation
```python
# Install PEFT library
# pip install peft

from peft import LoraConfig, get_peft_model

# Configure LoRA
lora_config = LoraConfig(
    r=64,  # Rank
    lora_alpha=128,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)

# Wrap model
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# Output: trainable params: 335,544,320 (0.48%)

# Train as normal but much faster
```

### Training Config
```yaml
# configs/cpt_lora_config.yaml
training:
  per_device_train_batch_size: 4  # 2x larger
  gradient_accumulation_steps: 8  # Reduced
  learning_rate: 1e-4  # Higher for LoRA
  max_steps: 20000
```

### Pros
- ‚úÖ Very fast training (4-6 hours)
- ‚úÖ Much lower memory (can use fewer GPUs)
- ‚úÖ Easy to merge adapters later
- ‚úÖ Can train multiple LoRAs for different legal areas

### Cons
- ‚ö†Ô∏è Not full model adaptation
- ‚ö†Ô∏è May not capture deep domain knowledge as well
- ‚ö†Ô∏è Additional step to merge adapters

### Best For
- Quick iterations
- Domain-specific fine-tuning
- Limited GPU budget
- Experimentation phase

---

## Option 5: Reduce Sequence Length

### Configuration
- **Current**: 4096 tokens per sequence
- **Reduced**: 2048 tokens per sequence
- **Time**: 10-12 hours (not quite 4-6h, but helpful)
- **Trade-off**: Less context per sequence

### Implementation
```yaml
# configs/cpt_config_short.yaml
data:
  max_seq_length: 2048  # Instead of 4096

training:
  per_device_train_batch_size: 4  # Can double batch size
```

### Pros
- ‚úÖ 2x more sequences processed
- ‚úÖ Can increase batch size
- ‚úÖ Slightly faster convergence

### Cons
- ‚ö†Ô∏è Less context for model to learn from
- ‚ö†Ô∏è May hurt long-document understanding
- ‚ö†Ô∏è Only ~40-50% time savings (not enough for 4-6h target)

---

## Option 6: Hybrid Approach (Recommended) ‚≠ê‚≠ê‚≠ê

**Combine multiple strategies for optimal balance**

### Configuration
1. **Reduced corpus**: Top 40% quality documents (~2B tokens)
2. **Reduced steps**: 12,000 steps
3. **Shorter sequences**: 3072 tokens (middle ground)
4. **Current GPUs**: 4x B200

### Expected Results
- **Time**: 4-6 hours
- **Quality**: 70-80% of full CPT
- **Cost**: Same as current ($150-200)

### Implementation
```yaml
# configs/cpt_config_hybrid_fast.yaml
model:
  name: "meta-llama/Llama-3.1-70B-Instruct"
  torch_dtype: "bfloat16"
  attn_implementation: "flash_attention_2"

training:
  max_steps: 12000  # 70% reduction
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 16
  learning_rate: 3e-5  # Slightly higher for faster adaptation

data:
  max_seq_length: 3072  # 25% reduction
  train_file: "data/cpt_filtered/cpt_train_shard_*.parquet"  # Filtered corpus
```

### Corpus Filtering Script
```python
# scripts/create_fast_cpt_corpus.py
"""Create high-quality subset for fast CPT"""

def filter_for_fast_training(documents, target_tokens=2_000_000_000):
    """
    Select highest quality documents for fast CPT

    Prioritization:
    1. Recent documents (2020+)
    2. Core legal areas (GDPR, consumer protection, environment)
    3. Regulations and directives
    4. Longer, substantive documents
    5. Frequently cited documents
    """
    scored_docs = []

    for doc in documents:
        score = calculate_quality_score(doc)
        scored_docs.append((score, doc))

    # Sort by score and select top documents
    scored_docs.sort(reverse=True)

    selected = []
    total_tokens = 0

    for score, doc in scored_docs:
        doc_tokens = estimate_tokens(doc['full_text'])
        if total_tokens + doc_tokens <= target_tokens:
            selected.append(doc)
            total_tokens += doc_tokens
        else:
            break

    return selected

def calculate_quality_score(doc):
    """Score document for training priority"""
    score = 0
    metadata = doc.get('metadata', {})

    # Recent documents (higher priority)
    date = metadata.get('date', '')
    if date:
        year = int(date[:4])
        if year >= 2020:
            score += 100
        elif year >= 2015:
            score += 50

    # Document type priority
    doc_type = metadata.get('doc_type', '')
    if doc_type == 'regulation':
        score += 80
    elif doc_type == 'directive':
        score += 60

    # Length (longer = more substantial)
    text_length = len(doc.get('full_text', ''))
    if text_length > 5000:
        score += 40
    elif text_length > 2000:
        score += 20

    # Subject matter (prioritize key areas)
    subjects = metadata.get('subject_matter', [])
    priority_subjects = ['data protection', 'consumer', 'environment', 'digital']
    for subject in subjects:
        if any(ps in subject.lower() for ps in priority_subjects):
            score += 30
            break

    return score
```

### Pros
- ‚úÖ Achieves 4-6 hour target
- ‚úÖ Maintains 70-80% of full CPT quality
- ‚úÖ No extra infrastructure cost
- ‚úÖ Focuses on most impactful content
- ‚úÖ Balanced trade-offs

### Cons
- ‚ö†Ô∏è Requires corpus filtering implementation
- ‚ö†Ô∏è Some domain coverage lost
- ‚ö†Ô∏è Slightly lower quality than full CPT

---

## Option 7: Two-Stage Training

### Configuration
- **Stage 1 (Fast)**: 4-6 hours, quick domain adaptation
- **Stage 2 (Optional)**: 15-20 hours, deep specialization
- **Total flexibility**: Start with Stage 1, add Stage 2 if needed

### Implementation
```yaml
# Stage 1: Fast CPT (4-6 hours)
# configs/cpt_stage1_fast.yaml
training:
  max_steps: 10000
  learning_rate: 3e-5  # Higher for faster adaptation
  output_dir: "./checkpoints/cpt_stage1"

# Stage 2: Deep CPT (optional, 15-20 hours)
# configs/cpt_stage2_deep.yaml
model:
  name: "./checkpoints/cpt_stage1/final"  # Resume from Stage 1
training:
  max_steps: 30000  # Additional steps
  learning_rate: 1e-5  # Lower for refinement
  output_dir: "./checkpoints/cpt_stage2"
```

### Workflow
1. Run Stage 1 (4-6 hours)
2. Evaluate on validation set
3. If quality sufficient ‚Üí proceed to SFT
4. If more needed ‚Üí run Stage 2

### Pros
- ‚úÖ Flexible approach
- ‚úÖ Quick initial results
- ‚úÖ Can always add more training later
- ‚úÖ Pay-as-you-go philosophy

### Cons
- ‚ö†Ô∏è Potential two-stage overhead
- ‚ö†Ô∏è Need to evaluate after Stage 1

---

## Comparison Matrix

| Option | Time | Cost | Quality | Complexity | Recommended |
|--------|------|------|---------|------------|-------------|
| **Reduced Steps** | 5-6h | $150 | 70% | Low | ‚≠ê‚≠ê‚≠ê |
| **Filtered Corpus** | 6h | $180 | 75% | Medium | ‚≠ê‚≠ê |
| **16x GPUs** | 5h | $2000 | 100% | High | ‚≠ê |
| **LoRA** | 4-6h | $150 | 60-70% | Medium | ‚≠ê‚≠ê |
| **Shorter Sequences** | 10-12h | $300 | 85% | Low | ‚≠ê |
| **Hybrid** | 4-6h | $180 | 75-80% | Medium | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Two-Stage** | 4-6h + opt | $150+ | 70-100% | Medium | ‚≠ê‚≠ê‚≠ê |

---

## Recommended Strategy

**For most use cases, the Hybrid Approach is best:**

1. **Filter corpus** to top 40% quality documents (2B tokens)
2. **Reduce steps** to 12,000
3. **Use 3072 token sequences** (slight reduction)
4. **Keep 4x B200 GPUs** (no extra cost)

**Expected Results:**
- ‚è±Ô∏è Training time: 4-6 hours
- üí∞ Cost: ~$180 (vs $500 for full)
- üìä Quality: 75-80% of full CPT
- ‚úÖ Good enough for most applications

**When to use alternatives:**

- **Budget critical?** ‚Üí Use LoRA (Option 4)
- **Quality critical?** ‚Üí Use 16 GPUs (Option 3) or Two-Stage (Option 7)
- **Prototyping?** ‚Üí Use Reduced Steps (Option 1)
- **Production?** ‚Üí Use Hybrid (Option 6) or Two-Stage (Option 7)

---

## Implementation Steps for Hybrid Approach

### Automated Workflow (Recommended)

Use the all-in-one script that handles filtering, corpus building, and training:

```bash
# Complete workflow: filtering ‚Üí corpus building ‚Üí training
./scripts/run_fast_cpt_training.sh
```

The script will:
1. Filter documents to top 40% quality (~2B tokens)
2. Build CPT corpus with 3072-token sequences
3. Transfer data to GPU cluster (if on Mac Studio)
4. Run fast CPT training (4-6 hours on GPU cluster)

**Configuration**: Uses `configs/cpt_config_fast.yaml` (already created)

### Manual Steps (Alternative)

If you prefer to run steps individually:

#### 1. Create Filtered Corpus
```bash
# Create high-quality subset
python scripts/create_fast_cpt_corpus.py \
  --input_dir data/parsed \
  --output_dir data/cpt_filtered/documents \
  --target_tokens 2000000000 \
  --quality_threshold 70
```

#### 2. Build CPT Corpus
```bash
# Build corpus with 3072-token sequences
python data_processing/dataset_builders/cpt_corpus_builder.py \
  --input_dir data/cpt_filtered/documents \
  --output_dir data/cpt_filtered \
  --max_seq_length 3072 \
  --num_shards 16
```

#### 3. Train (on GPU cluster)
```bash
export NVTE_FP8_DPA_BWD=1
export NVTE_ALLOW_NONDETERMINISTIC_ALGO=1

deepspeed --num_gpus=4 scripts/train_cpt.py \
  --config configs/cpt_config_fast.yaml \
  --deepspeed configs/ds_config_zero3.json \
  --use_fp8
```

### 4. Evaluate
```bash
# Quick validation check
python scripts/evaluate_cpt_checkpoint.py \
  --checkpoint checkpoints/cpt/checkpoint-12000 \
  --validation_data data/cpt_filtered/validation/
```

### 5. Proceed to SFT
If validation perplexity < 18, proceed with SFT training.

---

## Expected Quality Trade-offs

**Full CPT (20-24h):**
- Legal perplexity: ~12-15
- Domain adaptation: 100%
- Coverage: Comprehensive

**Fast CPT (4-6h):**
- Legal perplexity: ~15-18
- Domain adaptation: 70-80%
- Coverage: Core areas well-covered

**Impact on Final Model:**
- Citation accuracy: ~80-85% (vs 85-90% with full CPT)
- ROUGE-L: ~0.55-0.60 (vs 0.60-0.65 with full CPT)
- Still highly functional for most use cases

---

## Conclusion

**Best recommendation: Hybrid Approach (Option 6)**

Achieves your 4-6 hour target while maintaining 75-80% quality at the same cost. Perfect balance of speed, quality, and practicality.

For absolute best quality without time constraints, stick with the original 20-24 hour full CPT.
