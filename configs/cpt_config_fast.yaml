# Fast CPT Training Configuration (Hybrid Approach)
# Achieves 4-6 hour training time with 75-80% of full CPT quality
#
# Changes from standard CPT config:
# - Reduced steps: 12,000 (vs 40,000)
# - Shorter sequences: 3,072 tokens (vs 4,096)
# - Higher learning rate: 3e-5 (vs 2e-5) for faster adaptation
# - Filtered corpus: data/cpt_filtered/ (top 40% quality, ~2B tokens)
#
# Expected results:
# - Training time: 4-6 hours (vs 20-24 hours)
# - Cost: ~$180 (vs $500)
# - Quality: 75-80% of full CPT
# - Legal perplexity: ~15-18 (vs ~12-15 for full)

model:
  name: "meta-llama/Llama-3.3-70B-Instruct"
  torch_dtype: "bfloat16"
  attn_implementation: "flash_attention_2"
  use_cache: false

training:
  output_dir: "./checkpoints/cpt_fast"
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 16
  learning_rate: 3.0e-5  # Slightly higher for faster adaptation
  max_steps: 12000  # Step-based training: 70% reduction from 40K (covers ~2.4 epochs)
  lr_scheduler_type: "cosine"
  warmup_steps: 600  # Proportionally reduced (12K/40K * 2000)
  weight_decay: 0.01
  max_grad_norm: 1.0

  logging_steps: 10
  save_steps: 500  # More frequent checkpoints for shorter training
  eval_steps: 500
  save_total_limit: 5

  bf16: true
  fp8: true
  gradient_checkpointing: true
  deepspeed: "configs/ds_config_zero3.json"
  report_to: "wandb"

transformer_engine:
  enabled: true
  fp8_format: "hybrid"
  fp8_amax_history_len: 1024

data:
  # Use filtered corpus (created by scripts/create_fast_cpt_corpus.py)
  train_file: "data/cpt_filtered/train/cpt_train_shard_*.parquet"
  validation_file: "data/cpt_filtered/validation/cpt_val.parquet"
  max_seq_length: 3072  # 25% reduction from 4096
  preprocessing_num_workers: 8

wandb:
  project: "llama33-70b-eurlex"
  name: "cpt-fast-training"
  entity: null
  tags:
    - "fast-training"
    - "hybrid-approach"
    - "filtered-corpus"
