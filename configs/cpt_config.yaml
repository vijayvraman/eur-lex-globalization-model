model:
  name: "meta-llama/Llama-3.3-70B-Instruct"
  torch_dtype: "bfloat16"
  attn_implementation: "flash_attention_2"
  use_cache: false

training:
  output_dir: "./checkpoints/cpt"
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 16
  learning_rate: 2.0e-5
  max_steps: 4260  # 5 epochs over 446M tokens (balanced to prevent forgetting)
  lr_scheduler_type: "cosine"
  warmup_steps: 426  # 10% of max_steps
  weight_decay: 0.01
  max_grad_norm: 1.0

  logging_steps: 10
  save_steps: 1000
  eval_steps: 1000
  save_total_limit: 5

  bf16: true
  fp8: true
  gradient_checkpointing: true
  deepspeed: "configs/ds_config_zero3.json"
  report_to: "wandb"

transformer_engine:
  enabled: true
  fp8_format: "hybrid"
  fp8_amax_history_len: 1024

data:
  train_file: "data/cpt/train/cpt_train_shard_*.parquet"
  validation_file: "data/cpt/validation/cpt_val.parquet"
  max_seq_length: 4096
  preprocessing_num_workers: 8

wandb:
  project: "llama33-70b-eurlex"
  name: "cpt-training"
  entity: null
