model:
  name: "meta-llama/Llama-3.3-70B-Instruct"
  torch_dtype: "bfloat16"
  attn_implementation: "flash_attention_2"
  use_cache: false

training:
  output_dir: "./checkpoints/cpt"
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 16
  learning_rate: 2.0e-5
  max_steps: 4260  # 5 epochs over 446M tokens (balanced to prevent forgetting)
  lr_scheduler_type: "cosine"
  warmup_steps: 426  # 10% of max_steps
  weight_decay: 0.01
  max_grad_norm: 1.0

  logging_steps: 10
  save_steps: 1000
  eval_steps: 1000
  save_total_limit: 5

  bf16: true
  quantization: true  # Enable quantization via Transformer Engine
  gradient_checkpointing: true

  # Precision mode (Blackwell B200 GPUs)
  # fp8: 8-bit FP8 (E4M3/E5M2) - Default, stable, ~70GB for 70B model
  # nvfp4: 4-bit NVFP4 (E2M1) - Experimental, ~35GB for 70B model
  precision: "fp8"  # Default: fp8 (recommended)

  # Distributed training configuration
  # FSDP2 is the default (NVIDIA's recommended stack for Blackwell B200 GPUs)
  # Use --fsdp flag in training script (default) for FSDP2
  # Use --deepspeed flag for DeepSpeed ZeRO-3 (legacy compatibility)
  use_fsdp: true  # Default to FSDP2 (NVIDIA Blackwell optimized)
  fsdp_sharding_strategy: "full_shard"  # FULL_SHARD = ZeRO-3 equivalent
  deepspeed: "configs/ds_config_zero3.json"  # Used only when --deepspeed flag is set

  report_to: "wandb"

transformer_engine:
  enabled: true
  # FP8 settings (default precision mode)
  fp8_format: "hybrid"  # Hybrid: E4M3 for forward, E5M2 for backward
  fp8_amax_history_len: 1024  # AMAX history length for dynamic scaling
  # NVFP4 settings (experimental, use --precision nvfp4 to enable)
  nvfp4_block_size: 16  # Block size for NVFP4 scaling (16 or 32)

data:
  train_file: "data/cpt/train/cpt_train_shard_*.parquet"
  validation_file: "data/cpt/validation/cpt_val.parquet"
  max_seq_length: 4096
  preprocessing_num_workers: 8

wandb:
  project: "llama33-70b-eurlex"
  name: "cpt-training"
  entity: null
