model:
  name: "./models/llama33-70b-eurlex-cpt-final"
  torch_dtype: "bfloat16"
  attn_implementation: "flash_attention_2"
  use_cache: false

training:
  output_dir: "./checkpoints/sft"
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 8
  learning_rate: 5.0e-6
  num_train_epochs: 3
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.05
  weight_decay: 0.01
  max_grad_norm: 1.0

  logging_steps: 10
  save_steps: 500
  eval_steps: 500
  save_total_limit: 10
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"

  bf16: true
  quantization: true  # Enable quantization via Transformer Engine
  gradient_checkpointing: true

  # Precision mode (Blackwell B200 GPUs)
  # fp8: 8-bit FP8 (E4M3/E5M2) - Default, stable, ~70GB for 70B model
  # nvfp4: 4-bit NVFP4 (E2M1) - Experimental, ~35GB for 70B model
  precision: "fp8"  # Default: fp8 (recommended)

  # Distributed training configuration
  # FSDP2 is the default (NVIDIA's recommended stack for Blackwell B200 GPUs)
  # Use --fsdp flag in training script (default) for FSDP2
  # Use --deepspeed flag for DeepSpeed ZeRO-3 (legacy compatibility)
  use_fsdp: true  # Default to FSDP2 (NVIDIA Blackwell optimized)
  fsdp_sharding_strategy: "full_shard"  # FULL_SHARD = ZeRO-3 equivalent
  deepspeed: "configs/ds_config_zero3_sft.json"  # Used only when --deepspeed flag is set

  report_to: "wandb"

transformer_engine:
  enabled: true
  # FP8 settings (default precision mode)
  fp8_format: "hybrid"  # Hybrid: E4M3 for forward, E5M2 for backward
  fp8_amax_history_len: 1024  # AMAX history length for dynamic scaling
  # NVFP4 settings (experimental, use --precision nvfp4 to enable)
  nvfp4_block_size: 16  # Block size for NVFP4 scaling (16 or 32)

data:
  train_file: "data/sft/train/sft_train_shard_*.jsonl"
  validation_file: "data/sft/validation/sft_val.jsonl"
  max_seq_length: 2048
  preprocessing_num_workers: 8

sft:
  mask_inputs: true
  system_prompt: |
    You are a specialized legal assistant for European Union law. You provide accurate information about EU regulations, directives, and legal documents from EUR-Lex. Always cite specific articles and CELEX numbers when answering.

    Disclaimer: This is an AI assistant and does not constitute legal advice. Always consult official EUR-Lex sources and legal professionals for authoritative information.

wandb:
  project: "llama33-70b-eurlex"
  name: "sft-training"
  entity: null
